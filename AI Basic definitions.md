## Discriminative AI vs Generative AI
- Discriminative AI: It is inspired from Human behaviour of classification, clustering, analysis
- Generative AI: It is inspired from Human exhibits while creating Art, Music, Original designs, writing a story  or poem etc.
   - Eg: Chat `GPT-3` (for coding, text) & `DALL-E2` (for image generation tasks)

## Machine Learning vs Deep Learning
- There are significant error rate in Machine learning algorithms in image recognition and accurancy in classification.
- Inspired from Human brain functions, researches modelled a computing unit called the `Artificial Neuron`
- New architecture developed using Artifical neurons are called `Artifical Neural Networks` This model has three layers:
  - Input layer
  - Hidden layer
  - Output layer
- With increse in hidden layer, complex models can be built.
- Word `Deep` refers to large number of hidden layers in networks.
- With Artifical Neural Network accuracy incresed to 90%.
- Below illustrates the differnt networks:
<img width="1060" alt="Screenshot 2023-11-06 at 11 16 06 AM" src="https://github.com/Vamckis/ChatGPT/assets/71128825/9091351e-1b2b-4c2f-b0d8-a7742837bce0">

## Natural Language Processing (NLP):
- It is a disciplin of AL that helps machines to process text and speech data and generate responses. It can be used for below tasks:
  - Content categorisation
  - Sentiment Analysis
  - Document summarization
  - Context extraction
- E.g: chatbots on websites, IVR based Customer care (Interactive Voice Response)
- NLP has 2 components:
  - Natural Language understanding : To analyse spoken and written language.
  - Natural Language Generation: To generate responses.
 
  ## Large Language Models (LLMs)
- Pre-Trained Models are Machine learning models trained for specific purpose.
  - e.g: GPT, GPT-J, BLOOM, BERT are pre-trained Text models
  - e.g: AlexNet, GoogLeNet, OpenCV, Detectron2 are pre-trained image models.
- Large Language Models (LLMs) are Deep learning Models trained on huge corpus of data, thousands of GBs. They are neural networks with billions of parameters.
   - They can Summarize text, translate text, predixt text completion, generate code, explain code etc
   - e.g: Chat GPT-3 (It has 175 billions parameters, trained over 45TB of data), GPT-3.5, GPT-4.
   - e.g: Cohere Platform offers Generative language model and Representation language model.
   - e.g: M-TNLG (Megatron-Turing Natural Language Generation) had 530 billion aparameters developed by NVIDIA and Microsoft.
   - e.g: GPT-Neo, GPT-J, GPT-NeoX. These are traind on 825 GB of Pile dataset (Open Source language modelling dataset). These models work well when provided with few examples.
- Uses of LLMs:
  - Generate
  - Classify
  - Summarise
  - Rewrite
  - Search
  - Extract
  <img width="829" alt="Screenshot 2023-11-06 at 12 02 59 PM" src="https://github.com/Vamckis/Prompt-Engineering/assets/71128825/caec7213-2643-4557-bd6c-7a9143b4bca9">
